{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "ac5a4cf0-d9d2-47b5-9633-b53f8d99a4d2",
     "kernelId": ""
    },
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Perceiver Solo Piano (ver. 1.0)\n",
    "\n",
    "***\n",
    "\n",
    "Powered by tegridy-tools: https://github.com/asigalov61/tegridy-tools\n",
    "\n",
    "***\n",
    "\n",
    "WARNING: This complete implementation is a functioning model of the Artificial Intelligence. Please excercise great humility, care, and respect. https://www.nscai.gov/\n",
    "\n",
    "***\n",
    "\n",
    "#### Project Los Angeles\n",
    "\n",
    "#### Tegridy Code 2022\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fa0a611c-1803-42ae-bdf6-a49b5a4e781b",
     "kernelId": ""
    },
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "# (Setup Environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "editing": false,
     "id": "39411b40-9e39-416e-8fe4-d40f733e7956",
     "kernelId": ""
    },
    "id": "lw-4aqV3sKQG",
    "ExecuteTime": {
     "start_time": "2023-04-20T19:53:03.800581Z",
     "end_time": "2023-04-20T19:53:03.931637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "#@title nvidia-smi gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a1a45a91-d909-4fd4-b67a-5e16b971d179",
     "kernelId": ""
    },
    "id": "fX12Yquyuihc",
    "cellView": "form",
    "ExecuteTime": {
     "start_time": "2023-04-20T19:53:03.940511Z",
     "end_time": "2023-04-20T19:53:15.068005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Perceiver-Music-Transformer'...\r\n",
      "remote: Enumerating objects: 222, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (123/123), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (84/84), done.\u001B[K\r\n",
      "remote: Total 222 (delta 60), reused 74 (delta 36), pack-reused 99\u001B[Kobjects:  73% (163/222), 2.67 MiB | 2.60 MiB/sReceiving objects:  76% (169/222), 2.67 MiB | 2.60 MiB/s\r\n",
      "Receiving objects: 100% (222/222), 5.05 MiB | 3.57 MiB/s, done.\r\n",
      "Resolving deltas: 100% (96/96), done.\r\n",
      "Requirement already satisfied: einops in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (0.6.1)\r\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from torch) (4.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from torch) (3.12.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: torch-summary in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (1.4.5)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (4.65.0)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0mRequirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (1.0.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (4.39.3)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (5.12.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "The operation couldnâ€™t be completed. Unable to locate a Java Runtime.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n",
      "Requirement already satisfied: midi2audio in /opt/homebrew/Caskroom/miniforge/base/envs/chordResolution/lib/python3.9/site-packages (0.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "!git clone https://github.com/asigalov61/Perceiver-Music-Transformer\n",
    "!pip install einops\n",
    "!pip install torch\n",
    "!pip install torch-summary\n",
    "\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "\n",
    "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
    "!pip install midi2audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "gradient": {
     "editing": false,
     "id": "b8207b76-9514-4c07-95db-95a4742e52c5",
     "kernelId": ""
    },
    "id": "z7n9vnKmug1J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading needed modules. Please wait...\n",
      "Loading core modules...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Perceiver-Music-Transformer'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorchsummary\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m summary\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoading core modules...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/content/Perceiver-Music-Transformer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mTMIDIX\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mperceiver_ar_pytorch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PerceiverAR\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/Perceiver-Music-Transformer'"
     ]
    }
   ],
   "source": [
    "#@title Import all needed modules\n",
    "\n",
    "print('Loading needed modules. Please wait...')\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "print('Loading core modules...')\n",
    "os.chdir('/content/Perceiver-Music-Transformer')\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "from perceiver_ar_pytorch import PerceiverAR\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "from midi2audio import FluidSynth\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "os.chdir('/content/')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (DOWNLOAD MODEL)"
   ],
   "metadata": {
    "id": "iJ4xaSoA9A9s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Download Perceiver Pre-Trained Solo Piano Model\n",
    "!wget --no-check-certificate -O 'Perceiver-Solo-Piano-Model.pth' \"https://onedrive.live.com/download?cid=8A0D502FC99C608F&resid=8A0D502FC99C608F%2118753&authkey=AMmtup34-lfGqyA\""
   ],
   "metadata": {
    "id": "dICiWco0JwU4",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdKFoeke9L7H"
   },
   "source": [
    "# (LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load/Reload the model\n",
    "\n",
    "full_path_to_model_checkpoint = \"/content/Perceiver-Solo-Piano-Model.pth\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading the model...')\n",
    "# Load model\n",
    "\n",
    "# constants\n",
    "\n",
    "SEQ_LEN = 4096 * 4 # Total of 16k\n",
    "PREFIX_SEQ_LEN = (4096 * 4) - 1024 # 15.3k\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 24,\n",
    "    heads = 16,\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load(full_path_to_model_checkpoint)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# Model stats\n",
    "summary(model)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "-NLe35B0b9a5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (GENERATE)"
   ],
   "metadata": {
    "id": "RCKJSbl4erb0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Continuation"
   ],
   "metadata": {
    "id": "-OY_5PjpVJQ7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load Seed/Custom MIDI\n",
    "full_path_to_custom_MIDI_file = \"/content/Perceiver-Music-Transformer/Perceiver-Piano-Seed-1.mid\" #@param {type:\"string\"}\n",
    "\n",
    "print('Loading custom MIDI file...')\n",
    "score = TMIDIX.midi2ms_score(open(full_path_to_custom_MIDI_file, 'rb').read())\n",
    "\n",
    "events_matrix = []\n",
    "\n",
    "itrack = 1\n",
    "\n",
    "#==================================================\n",
    "\n",
    "# Memories augmentator\n",
    "\n",
    "def augment(inputs):\n",
    "\n",
    "  outs = []\n",
    "  outy = []\n",
    "\n",
    "  for i in range(1, 12):\n",
    "\n",
    "    out1 = []\n",
    "    out2 = []\n",
    "\n",
    "    for j in range(0, len(inputs), 4):\n",
    "      note = inputs[j:j+4]\n",
    "      aug_note1 = copy.deepcopy(note)\n",
    "      aug_note2 = copy.deepcopy(note)\n",
    "      aug_note1[2] += i\n",
    "      aug_note2[2] -= i\n",
    "\n",
    "      out1.append(aug_note1)\n",
    "      out2.append(aug_note2)\n",
    "\n",
    "    outs.append(out1[random.randint(0, int(len(out1) / 2)):random.randint(int(len(out1) / 2), len(out1))])\n",
    "    outs.append(out2[random.randint(0, int(len(out2) / 2)):random.randint(int(len(out2) / 2), len(out2))])\n",
    "\n",
    "  for i in range(64):\n",
    "    outy.extend(random.choice(outs))\n",
    "\n",
    "  outy1 = []\n",
    "  for o in outy:\n",
    "    outy1.extend(o)\n",
    "\n",
    "  return outy1\n",
    "\n",
    "#==================================================\n",
    "\n",
    "while itrack < len(score):\n",
    "    for event in score[itrack]:         \n",
    "        if event[0] == 'note' and event[3] != 9:\n",
    "            events_matrix.append(event)\n",
    "    itrack += 1\n",
    "\n",
    "if len(events_matrix) > 0:\n",
    "\n",
    "    # Sorting...\n",
    "    events_matrix.sort(key=lambda x: x[4], reverse=True)\n",
    "    events_matrix.sort(key=lambda x: x[1])\n",
    "\n",
    "    # recalculating timings\n",
    "    for e in events_matrix:\n",
    "        e[1] = int(e[1] / 10)\n",
    "        e[2] = int(e[2] / 20)\n",
    "\n",
    "    # final processing...\n",
    "    inputs = []\n",
    "    \n",
    "    inputs.extend([126+0, 126+128, 0+256, 0+384]) # Intro/Zero sequence\n",
    "\n",
    "    pe = events_matrix[0]\n",
    "    for e in events_matrix:\n",
    "\n",
    "        time = max(0, min(126, e[1]-pe[1]))\n",
    "        dur = max(1, min(126, e[2]))\n",
    "\n",
    "        ptc = max(1, min(126, e[4]))\n",
    "        vel = max(1, min(126, e[5]))\n",
    "\n",
    "        inputs.extend([time+0, dur+128, ptc+256, vel+384])\n",
    "\n",
    "        pe = e\n",
    "\n",
    "# =================================\n",
    "\n",
    "out1 = inputs\n",
    "\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = 0 # Piano\n",
    "\n",
    "        time += s[0] * 10\n",
    "            \n",
    "        dur = (s[1]-128) * 20\n",
    "        \n",
    "        pitch = (s[2]-256)\n",
    "\n",
    "        vel = (s[3]-384)\n",
    "\n",
    "        if pitch != 0:\n",
    "                                  \n",
    "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = '/content/Perceiver-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "print('Displaying resulting composition...')\n",
    "fname = '/content/Perceiver-Music-Composition'\n",
    "\n",
    "x = []\n",
    "y =[]\n",
    "c = []\n",
    "\n",
    "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
    "\n",
    "for s in song_f:\n",
    "  x.append(s[1] / 1000)\n",
    "  y.append(s[4])\n",
    "  c.append(colors[s[3]])\n",
    "\n",
    "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
    "display(Audio(str(fname + '.wav'), rate=16000))\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "ax=plt.axes(title=fname)\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "plt.scatter(x,y, c=c)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Pitch\")\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "dJaRwK9bUKwG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Continuation"
   ],
   "metadata": {
    "id": "aI0laUdWAkA3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Single Continuation Block Generator\n",
    "\n",
    "#@markdown NOTE: Play with the settings to get different results\n",
    "number_of_prime_tokens = 512 #@param {type:\"slider\", min:256, max:1020, step:4}\n",
    "number_of_tokens_to_generate = 512 #@param {type:\"slider\", min:64, max:512, step:32}\n",
    "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
    "\n",
    "#===================================================================\n",
    "print('=' * 70)\n",
    "print('Perceiver Music Model Continuation Generator')\n",
    "print('=' * 70)\n",
    "\n",
    "print('Generation settings:')\n",
    "print('=' * 70)\n",
    "print('Number of prime tokens:', number_of_prime_tokens)\n",
    "print('Number of tokens to generate:', number_of_tokens_to_generate)\n",
    "print('Model temperature:', temperature)\n",
    "\n",
    "print('=' * 70)\n",
    "print('Generating...')\n",
    "\n",
    "# inp = augment(inputs)\n",
    "\n",
    "inp = inputs * math.ceil((4096 * 4) / len(inputs))\n",
    "\n",
    "inp = inp[:(4096 * 4)]\n",
    "\n",
    "inp = inp[(512+len(inputs[:number_of_prime_tokens])):] + inputs[:number_of_prime_tokens]\n",
    "\n",
    "inp = torch.LongTensor(inp).cuda()\n",
    "\n",
    "out = model.generate(inp[None, ...], \n",
    "                     number_of_tokens_to_generate, \n",
    "                     temperature=temperature)  \n",
    "\n",
    "out1 = out.cpu().tolist()[0]\n",
    "\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = inputs[:number_of_prime_tokens] + out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = 0 # Piano\n",
    "\n",
    "        time += s[0] * 10\n",
    "            \n",
    "        dur = (s[1]-128) * 20\n",
    "        \n",
    "        pitch = (s[2]-256)\n",
    "\n",
    "        vel = (s[3]-384)\n",
    "\n",
    "        if pitch != 0:\n",
    "                                  \n",
    "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = '/content/Perceiver-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "print('Displaying resulting composition...')\n",
    "fname = '/content/Perceiver-Music-Composition'\n",
    "\n",
    "x = []\n",
    "y =[]\n",
    "c = []\n",
    "\n",
    "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
    "\n",
    "for s in song_f:\n",
    "  x.append(s[1] / 1000)\n",
    "  y.append(s[4])\n",
    "  c.append(colors[s[3]])\n",
    "\n",
    "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
    "display(Audio(str(fname + '.wav'), rate=16000))\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "ax=plt.axes(title=fname)\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "plt.scatter(x,y, c=c)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Pitch\")\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "bAWBH-MudV3t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Auto-Continue Custom MIDI\n",
    "\n",
    "number_of_continuation_notes = 100 #@param {type:\"slider\", min:10, max:500, step:10}\n",
    "number_of_prime_tokens = 256 #@param {type:\"slider\", min:64, max:512, step:4}\n",
    "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
    "\n",
    "#===================================================================\n",
    "print('=' * 70)\n",
    "print('Perceiver Music Model Auto-Continuation Generator')\n",
    "print('=' * 70)\n",
    "\n",
    "print('Generation settings:')\n",
    "print('=' * 70)\n",
    "print('Number of continuation notes:', number_of_continuation_notes)\n",
    "print('Number of prime tokens:', number_of_prime_tokens)\n",
    "print('Model temperature:', temperature)\n",
    "\n",
    "print('=' * 70)\n",
    "print('Generating...')\n",
    "\n",
    "out2 = copy.deepcopy(inputs[:number_of_prime_tokens])\n",
    "\n",
    "# aug_inp = augment(inputs)\n",
    "\n",
    "for i in tqdm(range(number_of_continuation_notes)):\n",
    "\n",
    "  # inp = copy.deepcopy(aug_inp)\n",
    "\n",
    "  inp = inputs * math.ceil((4096 * 4) / len(inputs))\n",
    "\n",
    "  inp = inp[:(4096 * 4)]\n",
    "\n",
    "  inp = inp[(512+len(out2)):] + out2\n",
    "\n",
    "  inp1 = torch.LongTensor(inp).cuda()\n",
    "\n",
    "  out = model.generate(inp1[None, ...], \n",
    "                      4, \n",
    "                      temperature=temperature)  \n",
    "\n",
    "  out1 = out.cpu().tolist()[0]\n",
    "  out2.extend(out1)\n",
    "\n",
    "if len(out2) != 0:\n",
    "    \n",
    "    song = out2\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = 0 # Piano\n",
    "\n",
    "        time += s[0] * 10\n",
    "            \n",
    "        dur = (s[1]-128) * 20\n",
    "        \n",
    "        pitch = (s[2]-256)\n",
    "\n",
    "        vel = (s[3]-384)\n",
    "\n",
    "        if pitch != 0:\n",
    "                                  \n",
    "          song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = '/content/Perceiver-Music-Composition', \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')\n",
    "\n",
    "print('Displaying resulting composition...')\n",
    "fname = '/content/Perceiver-Music-Composition'\n",
    "\n",
    "x = []\n",
    "y =[]\n",
    "c = []\n",
    "\n",
    "colors = ['red', 'yellow', 'green', 'cyan', 'blue', 'pink', 'orange', 'purple', 'gray', 'white', 'gold', 'silver']\n",
    "\n",
    "for s in song_f:\n",
    "  x.append(s[1] / 1000)\n",
    "  y.append(s[4])\n",
    "  c.append(colors[s[3]])\n",
    "\n",
    "FluidSynth(\"/usr/share/sounds/sf2/FluidR3_GM.sf2\", 16000).midi_to_audio(str(fname + '.mid'), str(fname + '.wav'))\n",
    "display(Audio(str(fname + '.wav'), rate=16000))\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "ax=plt.axes(title=fname)\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "plt.scatter(x,y, c=c)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Pitch\")\n",
    "plt.show()"
   ],
   "metadata": {
    "cellView": "form",
    "id": "TaH2748zN2oz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzCMd94Tu_gz"
   },
   "source": [
    "# Congrats! You did it! :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "gpuClass": "premium",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
